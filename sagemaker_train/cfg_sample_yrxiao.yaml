accum-freq: 4
beta1: 0.9
beta2: 0.95
# data-key: "json.gz"
dataset-resampled: False
# delete-previous-checkpoint: False
# Total 25B * 40 = 1T tokens
epochs: 1
fsdp: False
fsdp-limit-all-gathers: False
# grad-checkpointing: False
grad-clip-norm: 1
log-every-n-steps: 20
model: "open_lm_11m"
name: "open_lm_ex_$RANDOM"
precision: "amp_bfloat16"
report-to: "wandb"
seed: 124
# train-data-mix-weights: [0.725, 0.275]
# dataset-manifest: ["TODO"]
train-data: "../../open_lm/preproc_data_old/2048-v1/0/shard-{0000000..0000099}.tar"
train-num-samples: 1000000000
wandb-project-name: "lm7"
workers: 4
logs: ./ml/checkpoints/

# Some important parameters, double checked with Mitchell:
global-batch-size: 32
ffn-type: swiglu_torch
# fsdp-amp: False
fsdp-pure-bf16: False
fsdp-backward-prefetch: False
fsdp-use-orig-params: False
lr: 3.e-3
lr-cooldown-end: 3.e-5
model-norm: "gain_only_lp_layer_norm"
qk-norm: True
warmup: 2000
wd: 0.1
z-loss-coefficient: 1.e-4
attn-name: torch_attn
torchcompile: False
use-fp8: False
dist-backend: "xla"