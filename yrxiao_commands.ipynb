{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On TRN Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GELU, no Distributed Learning\n",
    "torchrun --nproc-per-node 1 -m open_lm.main   \\\n",
    " --model open_lm_11m \\\n",
    " --train-data /home/ubuntu/Workspace/open_lm/preproc_data_old/2048-v1/0/shard-{0000000..0000099}.tar \\\n",
    " --train-num-samples 1000000000 \\\n",
    " --workers 2 \\\n",
    " --dataset-resampled \\\n",
    " --precision amp_bfloat16 \\\n",
    " --global-batch-size 2 \\\n",
    " --log-every-n-steps 1 \\\n",
    " --grad-clip-norm 1 \\\n",
    " --data-key txt \\\n",
    " --lr 3e-4 \\\n",
    " --warmup 2000 \\\n",
    " --wd 0.1 \\\n",
    " --beta2 0.95 \\\n",
    " --epochs 100 \\\n",
    " --wandb-project-name open_lm_example \\\n",
    " --name open_lm_ex_$RANDOM \\\n",
    " --resume latest \\\n",
    " --logs ./logs/ \\\n",
    " --dist-backend xla \\\n",
    " --attn-name torch_attn \\\n",
    " --ffn-type gelu \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# swiglu_torch, no Distributed Learning\n",
    "torchrun --nproc-per-node 1 -m open_lm.main   \\\n",
    " --model open_lm_11m \\\n",
    " --train-data /home/ubuntu/Workspace/open_lm/preproc_data_old/2048-v1/0/shard-{0000000..0000099}.tar \\\n",
    " --train-num-samples 1000000000 \\\n",
    " --workers 2 \\\n",
    " --dataset-resampled \\\n",
    " --precision amp_bfloat16 \\\n",
    " --global-batch-size 2 \\\n",
    " --log-every-n-steps 1 \\\n",
    " --grad-clip-norm 1 \\\n",
    " --data-key txt \\\n",
    " --lr 3e-4 \\\n",
    " --warmup 2000 \\\n",
    " --wd 0.1 \\\n",
    " --beta2 0.95 \\\n",
    " --epochs 100 \\\n",
    " --wandb-project-name open_lm_example \\\n",
    " --name open_lm_ex_$RANDOM \\\n",
    " --resume latest \\\n",
    " --logs ./logs/ \\\n",
    " --dist-backend xla \\\n",
    " --attn-name torch_attn \\\n",
    " --ffn-type swiglu_torch \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GELU, no Distributed Learning ---  check speed\n",
    "# Speed checked ~9.6k/s/gpu\n",
    "torchrun --nproc-per-node 1 -m open_lm.main   \\\n",
    " --model open_lm_11m \\\n",
    " --train-data /home/ubuntu/Workspace/open_lm/preproc_data_old/2048-v1/0/shard-{0000000..0000099}.tar \\\n",
    " --train-num-samples 1000000000 \\\n",
    " --workers 2 \\\n",
    " --dataset-resampled \\\n",
    " --precision amp_bfloat16 \\\n",
    " --global-batch-size 2 \\\n",
    " --log-every-n-steps 100 \\\n",
    " --grad-clip-norm 1 \\\n",
    " --data-key txt \\\n",
    " --lr 3e-4 \\\n",
    " --warmup 2000 \\\n",
    " --wd 0.1 \\\n",
    " --beta2 0.95 \\\n",
    " --epochs 100 \\\n",
    " --wandb-project-name open_lm_example \\\n",
    " --name open_lm_ex_$RANDOM \\\n",
    " --resume latest \\\n",
    " --logs ./logs/ \\\n",
    " --dist-backend xla \\\n",
    " --attn-name torch_attn \\\n",
    " --ffn-type gelu \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GELU, Distributed Learning (with --grad-checkpointing )--- Working on / check speed\n",
    "# Speed checked ~9.6k/s/gpu\n",
    "torchrun --nproc-per-node 1 -m open_lm.main   \\\n",
    " --model open_lm_11m \\\n",
    " --train-data /home/ubuntu/Workspace/open_lm/preproc_data_old/2048-v1/0/shard-{0000000..0000099}.tar \\\n",
    " --train-num-samples 1000000000 \\\n",
    " --workers 2 \\\n",
    " --dataset-resampled \\\n",
    " --precision amp_bfloat16 \\\n",
    " --global-batch-size 2 \\\n",
    " --grad-checkpointing \\\n",
    " --log-every-n-steps 100 \\\n",
    " --grad-clip-norm 1 \\\n",
    " --data-key txt \\\n",
    " --lr 3e-4 \\\n",
    " --warmup 2000 \\\n",
    " --wd 0.1 \\\n",
    " --beta2 0.95 \\\n",
    " --epochs 100 \\\n",
    " --wandb-project-name open_lm_example \\\n",
    " --name open_lm_ex_$RANDOM \\\n",
    " --resume latest \\\n",
    " --logs ./logs/ \\\n",
    " --dist-backend xla \\\n",
    " --attn-name torch_attn \\\n",
    " --ffn-type gelu \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On CUDA machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchrun --nproc-per-node 1 -m open_lm.main   \\\n",
    " --model open_lm_11m \\\n",
    " --train-data /home/ubuntu/Workspace/open_lm/preproc_data_old/2048-v1/0/shard-{0000000..0000099}.tar \\\n",
    " --train-num-samples 1000000000 \\\n",
    " --workers 8 \\\n",
    " --dataset-resampled \\\n",
    " --precision amp_bfloat16 \\\n",
    " --global-batch-size 8 \\\n",
    " --log-every-n-steps 100 \\\n",
    " --grad-clip-norm 1 \\\n",
    " --data-key txt \\\n",
    " --lr 3e-4 \\\n",
    " --warmup 2000 \\\n",
    " --wd 0.1 \\\n",
    " --beta2 0.95 \\\n",
    " --epochs 100 \\\n",
    " --wandb-project-name open_lm_example \\\n",
    " --name open_lm_ex_$RANDOM \\\n",
    " --resume latest \\\n",
    " --logs ./logs/ \\\n",
    " --attn-name torch_attn \\\n",
    " --ffn-type gelu \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "/opt/conda/envs/open_lm/bin/torchrun --nproc-per-node 1 -m open_lm.main   \\\n",
    " --model open_lm_11m \\\n",
    " --train-data /home/ubuntu/Workspace/open_lm/preproc_data_old/2048-v1/0/shard-{0000000..0000099}.tar \\\n",
    " --train-num-samples 1000000000 \\\n",
    " --workers 2 \\\n",
    " --dataset-resampled \\\n",
    " --precision amp_bfloat16 \\\n",
    " --global-batch-size 2 \\\n",
    " --log-every-n-steps 1 \\\n",
    " --grad-clip-norm 1 \\\n",
    " --data-key txt \\\n",
    " --lr 3e-4 \\\n",
    " --warmup 2000 \\\n",
    " --wd 0.1 \\\n",
    " --beta2 0.95 \\\n",
    " --epochs 100 \\\n",
    " --wandb-project-name open_lm_example \\\n",
    " --name open_lm_ex_$RANDOM \\\n",
    " --resume latest \\\n",
    " --logs ./logs/ \\\n",
    " --attn-name torch_attn \\\n",
    " --ffn-type gelu \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GELU, no Distributed Learning --- Working on / check speed\n",
    "# Speed checked ~10k/s/gpu\n",
    "/opt/conda/envs/open_lm/bin/torchrun --nproc-per-node 1 -m open_lm.main   \\\n",
    " --model open_lm_11m \\\n",
    " --train-data /home/ubuntu/Workspace/open_lm/preproc_data_old/2048-v1/0/shard-{0000000..0000099}.tar \\\n",
    " --train-num-samples 1000000000 \\\n",
    " --workers 8 \\\n",
    " --dataset-resampled \\\n",
    " --precision amp_bfloat16 \\\n",
    " --global-batch-size 8 \\\n",
    " --log-every-n-steps 1 \\\n",
    " --grad-clip-norm 1 \\\n",
    " --data-key txt \\\n",
    " --lr 3e-4 \\\n",
    " --warmup 2000 \\\n",
    " --wd 0.1 \\\n",
    " --beta2 0.95 \\\n",
    " --epochs 100 \\\n",
    " --wandb-project-name open_lm_example \\\n",
    " --name open_lm_ex_$RANDOM \\\n",
    " --resume latest \\\n",
    " --logs ./logs/ \\\n",
    " --attn-name torch_attn \\\n",
    " --ffn-type gelu \\"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
