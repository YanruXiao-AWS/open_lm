{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On TRN Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GELU, no Distributed Learning\n",
    "torchrun --nproc-per-node 1 -m open_lm.main   \\\n",
    " --model open_lm_11m \\\n",
    " --train-data /home/ubuntu/Workspace/open_lm/preproc_data_old/2048-v1/0/shard-{0000000..0000099}.tar \\\n",
    " --train-num-samples 1000000000 \\\n",
    " --workers 2 \\\n",
    " --dataset-resampled \\\n",
    " --precision amp_bfloat16 \\\n",
    " --global-batch-size 2 \\\n",
    " --log-every-n-steps 1 \\\n",
    " --grad-clip-norm 1 \\\n",
    " --data-key txt \\\n",
    " --lr 3e-4 \\\n",
    " --warmup 2000 \\\n",
    " --wd 0.1 \\\n",
    " --beta2 0.95 \\\n",
    " --epochs 100 \\\n",
    " --wandb-project-name open_lm_example \\\n",
    " --name open_lm_ex_$RANDOM \\\n",
    " --resume latest \\\n",
    " --logs ./logs/ \\\n",
    " --dist-backend xla \\\n",
    " --attn-name torch_attn \\\n",
    " --ffn-type gelu \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# swiglu_torch, no Distributed Learning\n",
    "torchrun --nproc-per-node 1 -m open_lm.main   \\\n",
    " --model open_lm_11m \\\n",
    " --train-data /home/ubuntu/Workspace/open_lm/preproc_data_old/2048-v1/0/shard-{0000000..0000099}.tar \\\n",
    " --train-num-samples 1000000000 \\\n",
    " --workers 2 \\\n",
    " --dataset-resampled \\\n",
    " --precision amp_bfloat16 \\\n",
    " --global-batch-size 2 \\\n",
    " --log-every-n-steps 1 \\\n",
    " --grad-clip-norm 1 \\\n",
    " --data-key txt \\\n",
    " --lr 3e-4 \\\n",
    " --warmup 2000 \\\n",
    " --wd 0.1 \\\n",
    " --beta2 0.95 \\\n",
    " --epochs 100 \\\n",
    " --wandb-project-name open_lm_example \\\n",
    " --name open_lm_ex_$RANDOM \\\n",
    " --resume latest \\\n",
    " --logs ./logs/ \\\n",
    " --dist-backend xla \\\n",
    " --attn-name torch_attn \\\n",
    " --ffn-type swiglu_torch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distributed, -  work\n",
    "# working but only 2 nodes (can see NC0 and NC1 usage)\n",
    "# 22k/s\n",
    "torchrun --nproc-per-node 2 -m open_lm.main   \\\n",
    " --model open_lm_11m \\\n",
    " --train-data /home/ubuntu/Workspace/open_lm/preproc_data_old/2048-v1/0/shard-{0000000..0000099}.tar \\\n",
    " --train-num-samples 1000000000 \\\n",
    " --workers 4 \\\n",
    " --dataset-resampled \\\n",
    " --precision amp_bfloat16 \\\n",
    " --global-batch-size 4 \\\n",
    " --log-every-n-steps 100 \\\n",
    " --grad-clip-norm 1 \\\n",
    " --data-key txt \\\n",
    " --lr 3e-4 \\\n",
    " --warmup 2000 \\\n",
    " --wd 0.1 \\\n",
    " --beta2 0.95 \\\n",
    " --epochs 100 \\\n",
    " --wandb-project-name open_lm_example \\\n",
    " --name open_lm_ex_$RANDOM \\\n",
    " --resume latest \\\n",
    " --logs ./logs/ \\\n",
    " --dist-backend xla \\\n",
    " --attn-name torch_attn \\\n",
    " --ffn-type swiglu_torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distributed, -  work\n",
    "# working but only 2 nodes (can see NC0 and NC1 usage)\n",
    "# 22k/s - interesting / slower than the 4 batch size\n",
    "torchrun --nproc-per-node 2 -m open_lm.main   \\\n",
    " --model open_lm_11m \\\n",
    " --train-data /home/ubuntu/Workspace/open_lm/preproc_data_old/2048-v1/0/shard-{0000000..0000099}.tar \\\n",
    " --train-num-samples 1000000000 \\\n",
    " --workers 4 \\\n",
    " --dataset-resampled \\\n",
    " --precision amp_bfloat16 \\\n",
    " --global-batch-size 8 \\\n",
    " --log-every-n-steps 100 \\\n",
    " --grad-clip-norm 1 \\\n",
    " --data-key txt \\\n",
    " --lr 3e-4 \\\n",
    " --warmup 2000 \\\n",
    " --wd 0.1 \\\n",
    " --beta2 0.95 \\\n",
    " --epochs 100 \\\n",
    " --wandb-project-name open_lm_example \\\n",
    " --name open_lm_ex_$RANDOM \\\n",
    " --resume latest \\\n",
    " --logs ./logs/ \\\n",
    " --dist-backend xla \\\n",
    " --attn-name torch_attn \\\n",
    " --ffn-type swiglu_torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distributed Compile only, -  not working\n",
    "# \n",
    "neuron_parallel_compile torchrun --nproc-per-node 2 -m open_lm.main   \\\n",
    " --model open_lm_11m \\\n",
    " --train-data /home/ubuntu/Workspace/open_lm/preproc_data_old/2048-v1/0/shard-{0000000..0000099}.tar \\\n",
    " --train-num-samples 1000000000 \\\n",
    " --workers 4 \\\n",
    " --dataset-resampled \\\n",
    " --precision amp_bfloat16 \\\n",
    " --global-batch-size 8 \\\n",
    " --log-every-n-steps 100 \\\n",
    " --grad-clip-norm 1 \\\n",
    " --data-key txt \\\n",
    " --lr 3e-4 \\\n",
    " --warmup 2000 \\\n",
    " --wd 0.1 \\\n",
    " --beta2 0.95 \\\n",
    " --epochs 100 \\\n",
    " --wandb-project-name open_lm_example \\\n",
    " --name open_lm_ex_$RANDOM \\\n",
    " --resume latest \\\n",
    " --logs ./logs/ \\\n",
    " --dist-backend xla \\\n",
    " --attn-name torch_attn \\\n",
    " --ffn-type swiglu_torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchrun --nproc-per-node 32 -m open_lm.main   \\\n",
    " --model open_lm_11m \\\n",
    " --train-data /home/ubuntu/Workspace/open_lm/preproc_data_old/2048-v1/0/shard-{0000000..0000099}.tar \\\n",
    " --train-num-samples 1000000000 \\\n",
    " --workers 32 \\\n",
    " --dataset-resampled \\\n",
    " --precision amp_bfloat16 \\\n",
    " --global-batch-size 64 \\\n",
    " --log-every-n-steps 100 \\\n",
    " --grad-clip-norm 1 \\\n",
    " --data-key txt \\\n",
    " --lr 3e-4 \\\n",
    " --warmup 2000 \\\n",
    " --wd 0.1 \\\n",
    " --beta2 0.95 \\\n",
    " --epochs 100 \\\n",
    " --wandb-project-name open_lm_example \\\n",
    " --name open_lm_ex_$RANDOM \\\n",
    " --resume latest \\\n",
    " --logs ./logs/ \\\n",
    " --dist-backend xla \\\n",
    " --attn-name torch_attn \\\n",
    " --ffn-type swiglu_torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi device training - 11m \n",
    "# Should work for now\n",
    "torchrun --nproc-per-node 2 -m open_lm.main   \\\n",
    " --model open_lm_11m \\\n",
    " --train-data /home/ubuntu/Workspace/open_lm/preproc_data_old/2048-v1/0/shard-{0000000..0000004}.tar \\\n",
    " --train-num-samples 1000000000 \\\n",
    " --workers 4 \\\n",
    " --dataset-resampled \\\n",
    " --precision amp_bfloat16 \\\n",
    " --global-batch-size 8 \\\n",
    " --log-every-n-steps 100 \\\n",
    " --grad-clip-norm 1 \\\n",
    " --data-key txt \\\n",
    " --lr 3e-4 \\\n",
    " --warmup 2000 \\\n",
    " --wd 0.1 \\\n",
    " --beta2 0.95 \\\n",
    " --epochs 100 \\\n",
    " --wandb-project-name open_lm_example \\\n",
    " --name open_lm_ex_$RANDOM \\\n",
    " --resume latest \\\n",
    " --logs ./logs/ \\\n",
    " --dist-backend xla \\\n",
    " --attn-name torch_attn \\\n",
    " --ffn-type swiglu_torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi device compile - 8 node\n",
    "neuron_parallel_compile torchrun --nproc-per-node 8 -m open_lm.main   \\\n",
    " --model open_lm_11m \\\n",
    " --train-data /home/ubuntu/Workspace/open_lm/preproc_data_old/2048-v1/0/shard-{0000000..0000099}.tar \\\n",
    " --train-num-samples 10000 \\\n",
    " --workers 8 \\\n",
    " --dataset-resampled \\\n",
    " --precision amp_bfloat16 \\\n",
    " --global-batch-size 8 \\\n",
    " --log-every-n-steps 100 \\\n",
    " --grad-clip-norm 1 \\\n",
    " --data-key txt \\\n",
    " --lr 3e-4 \\\n",
    " --warmup 2000 \\\n",
    " --wd 0.1 \\\n",
    " --beta2 0.95 \\\n",
    " --epochs 100 \\\n",
    " --wandb-project-name open_lm_example \\\n",
    " --name open_lm_ex_$RANDOM \\\n",
    " --resume latest \\\n",
    " --logs ./logs/ \\\n",
    " --dist-backend xla \\\n",
    " --attn-name torch_attn \\\n",
    " --ffn-type swiglu_torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchrun --nproc-per-node 1 -m open_lm.main   \\\n",
    " --model open_lm_11m \\\n",
    " --train-data /home/ubuntu/Workspace/open_lm/preproc_data_old/2048-v1/0/shard-{0000000..0000099}.tar \\\n",
    " --train-num-samples 1000000000 \\\n",
    " --workers 4 \\\n",
    " --dataset-resampled \\\n",
    " --precision amp_bfloat16 \\\n",
    " --global-batch-size 4 \\\n",
    " --log-every-n-steps 100 \\\n",
    " --grad-clip-norm 1 \\\n",
    " --data-key txt \\\n",
    " --lr 3e-4 \\\n",
    " --warmup 2000 \\\n",
    " --wd 0.1 \\\n",
    " --beta2 0.95 \\\n",
    " --epochs 100 \\\n",
    " --wandb-project-name open_lm_example \\\n",
    " --name open_lm_ex_$RANDOM \\\n",
    " --resume latest \\\n",
    " --logs ./logs/ \\\n",
    " --dist-backend xla \\\n",
    " --attn-name torch_attn \\\n",
    " --ffn-type swiglu_torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchrun --nproc-per-node 4 -m open_lm.main   \\\n",
    " --model open_lm_11m \\\n",
    " --train-data /home/ubuntu/Workspace/open_lm/preproc_data_old/2048-v1/0/shard-{0000000..0000099}.tar \\\n",
    " --train-num-samples 1000000000 \\\n",
    " --workers 4 \\\n",
    " --dataset-resampled \\\n",
    " --precision amp_bfloat16 \\\n",
    " --global-batch-size 4 \\\n",
    " --log-every-n-steps 1 \\\n",
    " --grad-clip-norm 1 \\\n",
    " --data-key txt \\\n",
    " --lr 3e-4 \\\n",
    " --warmup 2000 \\\n",
    " --wd 0.1 \\\n",
    " --beta2 0.95 \\\n",
    " --epochs 100 \\\n",
    " --wandb-project-name open_lm_example \\\n",
    " --name open_lm_ex_$RANDOM \\\n",
    " --resume latest \\\n",
    " --logs ./logs/ \\\n",
    " --dist-backend xla \\\n",
    " --attn-name torch_attn \\\n",
    " --ffn-type swiglu_torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distributed, no resume latest - not working\n",
    "torchrun --nproc-per-node 2 -m open_lm.main   \\\n",
    " --model open_lm_11m \\\n",
    " --train-data /home/ubuntu/Workspace/open_lm/preproc_data_old/2048-v1/0/shard-{0000000..0000099}.tar \\\n",
    " --train-num-samples 1000000000 \\\n",
    " --workers 2 \\\n",
    " --dataset-resampled \\\n",
    " --precision amp_bfloat16 \\\n",
    " --global-batch-size 2 \\\n",
    " --log-every-n-steps 10 \\\n",
    " --grad-clip-norm 1 \\\n",
    " --data-key txt \\\n",
    " --lr 3e-4 \\\n",
    " --warmup 2000 \\\n",
    " --wd 0.1 \\\n",
    " --beta2 0.95 \\\n",
    " --epochs 100 \\\n",
    " --wandb-project-name open_lm_example \\\n",
    " --name open_lm_ex_$RANDOM \\\n",
    " --logs ./logs/ \\\n",
    " --dist-backend xla \\\n",
    " --attn-name torch_attn \\\n",
    " --ffn-type swiglu_torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distributed, work - TP only\n",
    "# \n",
    "torchrun --nproc-per-node 2 -m open_lm.main   \\\n",
    " --model open_lm_11m \\\n",
    " --train-data /home/ubuntu/Workspace/open_lm/preproc_data_old/2048-v1/0/shard-{0000000..0000004}.tar \\\n",
    " --train-num-samples 1000000000 \\\n",
    " --workers 4 \\\n",
    " --dataset-resampled \\\n",
    " --precision amp_bfloat16 \\\n",
    " --global-batch-size 1 \\\n",
    " --log-every-n-steps 100 \\\n",
    " --grad-clip-norm 1 \\\n",
    " --data-key txt \\\n",
    " --lr 3e-4 \\\n",
    " --warmup 2000 \\\n",
    " --wd 0.1 \\\n",
    " --beta2 0.95 \\\n",
    " --epochs 100 \\\n",
    " --wandb-project-name open_lm_example \\\n",
    " --name open_lm_ex_$RANDOM \\\n",
    " --resume latest \\\n",
    " --logs ./logs/ \\\n",
    " --dist-backend xla \\\n",
    " --attn-name torch_attn \\\n",
    " --ffn-type swiglu_torch \\\n",
    " --tensor-parallel-size 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distributed, work - TP with DP\n",
    "# \n",
    "# \n",
    "\n",
    "# NEURON_CC_FLAGS=\" --log_level=ERROR --cache_dir=../compiler_cache -O1\"\n",
    "# neuron_parallel_compile \\\n",
    "torchrun --nproc-per-node 2 -m open_lm.main   \\\n",
    " --model open_lm_11m \\\n",
    " --train-data /home/ubuntu/Workspace/open_lm/preproc_data_old/2048-v1/0/shard-{0000000..0000004}.tar \\\n",
    " --train-num-samples 1000000000 \\\n",
    " --workers 4 \\\n",
    " --dataset-resampled \\\n",
    " --precision amp_bfloat16 \\\n",
    " --global-batch-size 2 \\\n",
    " --log-every-n-steps 100 \\\n",
    " --grad-clip-norm 1 \\\n",
    " --data-key txt \\\n",
    " --lr 3e-4 \\\n",
    " --warmup 2000 \\\n",
    " --wd 0.1 \\\n",
    " --beta2 0.95 \\\n",
    " --epochs 100 \\\n",
    " --wandb-project-name open_lm_example \\\n",
    " --name open_lm_ex_$RANDOM \\\n",
    " --resume latest \\\n",
    " --logs ./logs/ \\\n",
    " --dist-backend xla \\\n",
    " --attn-name torch_attn \\\n",
    " --ffn-type swiglu_torch \\\n",
    " --tensor-parallel-size 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GELU - Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GELU, no Distributed Learning ---  check speed\n",
    "# Speed checked ~9.6k/s/gpu\n",
    "torchrun --nproc-per-node 1 -m open_lm.main   \\\n",
    " --model open_lm_11m \\\n",
    " --train-data /home/ubuntu/Workspace/open_lm/preproc_data_old/2048-v1/0/shard-{0000000..0000099}.tar \\\n",
    " --train-num-samples 1000000000 \\\n",
    " --workers 2 \\\n",
    " --dataset-resampled \\\n",
    " --precision amp_bfloat16 \\\n",
    " --global-batch-size 2 \\\n",
    " --log-every-n-steps 100 \\\n",
    " --grad-clip-norm 1 \\\n",
    " --data-key txt \\\n",
    " --lr 3e-4 \\\n",
    " --warmup 2000 \\\n",
    " --wd 0.1 \\\n",
    " --beta2 0.95 \\\n",
    " --epochs 100 \\\n",
    " --wandb-project-name open_lm_example \\\n",
    " --name open_lm_ex_$RANDOM \\\n",
    " --resume latest \\\n",
    " --logs ./logs/ \\\n",
    " --dist-backend xla \\\n",
    " --attn-name torch_attn \\\n",
    " --ffn-type gelu \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GELU, Distributed Learning (with --grad-checkpointing )--- not working\n",
    "# Speed checked ~9.6k/s/gpu\n",
    "torchrun --nproc-per-node 2 -m open_lm.main   \\\n",
    " --model open_lm_11m \\\n",
    " --train-data /home/ubuntu/Workspace/open_lm/preproc_data_old/2048-v1/0/shard-{0000000..0000099}.tar \\\n",
    " --train-num-samples 1000000000 \\\n",
    " --workers 2 \\\n",
    " --dataset-resampled \\\n",
    " --precision amp_bfloat16 \\\n",
    " --global-batch-size 2 \\\n",
    " --grad-checkpointing \\\n",
    " --log-every-n-steps 100 \\\n",
    " --grad-clip-norm 1 \\\n",
    " --data-key txt \\\n",
    " --lr 3e-4 \\\n",
    " --warmup 2000 \\\n",
    " --wd 0.1 \\\n",
    " --beta2 0.95 \\\n",
    " --epochs 100 \\\n",
    " --wandb-project-name open_lm_example \\\n",
    " --name open_lm_ex_$RANDOM \\\n",
    " --resume latest \\\n",
    " --logs ./logs/ \\\n",
    " --dist-backend xla \\\n",
    " --attn-name torch_attn \\\n",
    " --ffn-type gelu \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On CUDA machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchrun --nproc-per-node 1 -m open_lm.main   \\\n",
    " --model open_lm_11m \\\n",
    " --train-data /home/ubuntu/Workspace/open_lm/preproc_data_old/2048-v1/0/shard-{0000000..0000099}.tar \\\n",
    " --train-num-samples 1000000000 \\\n",
    " --workers 8 \\\n",
    " --dataset-resampled \\\n",
    " --precision amp_bfloat16 \\\n",
    " --global-batch-size 8 \\\n",
    " --log-every-n-steps 100 \\\n",
    " --grad-clip-norm 1 \\\n",
    " --data-key txt \\\n",
    " --lr 3e-4 \\\n",
    " --warmup 2000 \\\n",
    " --wd 0.1 \\\n",
    " --beta2 0.95 \\\n",
    " --epochs 100 \\\n",
    " --wandb-project-name open_lm_example \\\n",
    " --name open_lm_ex_$RANDOM \\\n",
    " --resume latest \\\n",
    " --logs ./logs/ \\\n",
    " --attn-name torch_attn \\\n",
    " --ffn-type gelu \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "/opt/conda/envs/open_lm/bin/torchrun --nproc-per-node 1 -m open_lm.main   \\\n",
    " --model open_lm_11m \\\n",
    " --train-data /home/ubuntu/Workspace/open_lm/preproc_data_old/2048-v1/0/shard-{0000000..0000099}.tar \\\n",
    " --train-num-samples 1000000000 \\\n",
    " --workers 2 \\\n",
    " --dataset-resampled \\\n",
    " --precision amp_bfloat16 \\\n",
    " --global-batch-size 2 \\\n",
    " --log-every-n-steps 1 \\\n",
    " --grad-clip-norm 1 \\\n",
    " --data-key txt \\\n",
    " --lr 3e-4 \\\n",
    " --warmup 2000 \\\n",
    " --wd 0.1 \\\n",
    " --beta2 0.95 \\\n",
    " --epochs 100 \\\n",
    " --wandb-project-name open_lm_example \\\n",
    " --name open_lm_ex_$RANDOM \\\n",
    " --resume latest \\\n",
    " --logs ./logs/ \\\n",
    " --attn-name torch_attn \\\n",
    " --ffn-type gelu \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GELU, no Distributed Learning --- Working on / check speed\n",
    "# Speed checked ~10k/s/gpu\n",
    "/opt/conda/envs/open_lm/bin/torchrun --nproc-per-node 1 -m open_lm.main   \\\n",
    " --model open_lm_11m \\\n",
    " --train-data /home/ubuntu/Workspace/open_lm/preproc_data_old/2048-v1/0/shard-{0000000..0000099}.tar \\\n",
    " --train-num-samples 1000000000 \\\n",
    " --workers 8 \\\n",
    " --dataset-resampled \\\n",
    " --precision amp_bfloat16 \\\n",
    " --global-batch-size 8 \\\n",
    " --log-every-n-steps 1 \\\n",
    " --grad-clip-norm 1 \\\n",
    " --data-key txt \\\n",
    " --lr 3e-4 \\\n",
    " --warmup 2000 \\\n",
    " --wd 0.1 \\\n",
    " --beta2 0.95 \\\n",
    " --epochs 100 \\\n",
    " --wandb-project-name open_lm_example \\\n",
    " --name open_lm_ex_$RANDOM \\\n",
    " --resume latest \\\n",
    " --logs ./logs/ \\\n",
    " --attn-name torch_attn \\\n",
    " --ffn-type gelu \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Swiglu torch, Distributed Training - Working. \n",
    "/opt/conda/envs/open_lm/bin/torchrun --nproc-per-node 2 -m open_lm.main   \\\n",
    " --model open_lm_11m \\\n",
    " --train-data /home/ubuntu/Workspace/open_lm/preproc_data_old/2048-v1/0/shard-{0000000..0000099}.tar \\\n",
    " --train-num-samples 1000000000 \\\n",
    " --workers 2 \\\n",
    " --dataset-resampled \\\n",
    " --precision amp_bfloat16 \\\n",
    " --global-batch-size 2 \\\n",
    " --log-every-n-steps 1 \\\n",
    " --grad-clip-norm 1 \\\n",
    " --data-key txt \\\n",
    " --lr 3e-4 \\\n",
    " --warmup 2000 \\\n",
    " --wd 0.1 \\\n",
    " --beta2 0.95 \\\n",
    " --epochs 100 \\\n",
    " --wandb-project-name open_lm_example \\\n",
    " --name open_lm_ex_$RANDOM \\\n",
    " --resume latest \\\n",
    " --logs ./logs/ \\\n",
    " --attn-name torch_attn \\\n",
    " --ffn-type swiglu_torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Swiglu torch, Distributed Training - Working. \n",
    "# 4 gpu - batchsize 4\n",
    "# Speed 160k /s , 40k /gpu\n",
    "/opt/conda/envs/open_lm/bin/torchrun --nproc-per-node 4 -m open_lm.main   \\\n",
    " --model open_lm_11m \\\n",
    " --train-data /home/ubuntu/Workspace/open_lm/preproc_data_old/2048-v1/0/shard-{0000000..0000004}.tar \\\n",
    " --train-num-samples 1000000000 \\\n",
    " --workers 2 \\\n",
    " --dataset-resampled \\\n",
    " --precision amp_bfloat16 \\\n",
    " --global-batch-size 4 \\\n",
    " --log-every-n-steps 100 \\\n",
    " --grad-clip-norm 1 \\\n",
    " --data-key txt \\\n",
    " --lr 3e-4 \\\n",
    " --warmup 2000 \\\n",
    " --wd 0.1 \\\n",
    " --beta2 0.95 \\\n",
    " --epochs 100 \\\n",
    " --wandb-project-name open_lm_example \\\n",
    " --name open_lm_ex_$RANDOM \\\n",
    " --resume latest \\\n",
    " --logs ./logs/ \\\n",
    " --attn-name torch_attn \\\n",
    " --ffn-type swiglu_torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Swiglu torch, Distributed Training - Working. \n",
    "# 4 gpu - batchsize 8\n",
    "# Speed  240k/s ,  60k/gpu\n",
    "/opt/conda/envs/open_lm/bin/torchrun --nproc-per-node 4 -m open_lm.main   \\\n",
    " --model open_lm_11m \\\n",
    " --train-data /home/ubuntu/Workspace/open_lm/preproc_data_old/2048-v1/0/shard-{0000000..0000004}.tar \\\n",
    " --train-num-samples 1000000000 \\\n",
    " --workers 8 \\\n",
    " --dataset-resampled \\\n",
    " --precision amp_bfloat16 \\\n",
    " --global-batch-size 16 \\\n",
    " --log-every-n-steps 100 \\\n",
    " --grad-clip-norm 1 \\\n",
    " --data-key txt \\\n",
    " --lr 3e-4 \\\n",
    " --warmup 2000 \\\n",
    " --wd 0.1 \\\n",
    " --beta2 0.95 \\\n",
    " --epochs 100 \\\n",
    " --wandb-project-name open_lm_example \\\n",
    " --name open_lm_ex_$RANDOM \\\n",
    " --resume latest \\\n",
    " --logs ./logs/ \\\n",
    " --attn-name torch_attn \\\n",
    " --ffn-type swiglu_torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking - 1B model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not runnable - not enough gpu memory\n",
    "/opt/conda/envs/open_lm/bin/torchrun --nproc-per-node 4 -m open_lm.main   \\\n",
    " --model open_lm_1b \\\n",
    " --train-data /home/ubuntu/Workspace/open_lm/preproc_data_old/2048-v1/0/shard-{0000000..0000004}.tar \\\n",
    " --train-num-samples 1000000000 \\\n",
    " --workers 8 \\\n",
    " --dataset-resampled \\\n",
    " --precision amp_bfloat16 \\\n",
    " --global-batch-size 4 \\\n",
    " --log-every-n-steps 100 \\\n",
    " --grad-clip-norm 1 \\\n",
    " --data-key txt \\\n",
    " --lr 3e-4 \\\n",
    " --warmup 2000 \\\n",
    " --wd 0.1 \\\n",
    " --beta2 0.95 \\\n",
    " --epochs 100 \\\n",
    " --wandb-project-name open_lm_example \\\n",
    " --name open_lm_ex_$RANDOM \\\n",
    " --resume latest \\\n",
    " --logs ./logs/ \\\n",
    " --attn-name torch_attn \\\n",
    " --ffn-type swiglu_torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not runnable - not enough gpu memory\n",
    "/opt/conda/envs/open_lm/bin/torchrun --nproc-per-node 4 -m open_lm.main   \\\n",
    " --model open_lm_1b \\\n",
    " --train-data /home/ubuntu/Workspace/open_lm/preproc_data_old/2048-v1/0/shard-{0000000..0000004}.tar \\\n",
    " --train-num-samples 1000000000 \\\n",
    " --workers 8 \\\n",
    " --dataset-resampled \\\n",
    " --precision amp_bfloat16 \\\n",
    " --global-batch-size 4 \\\n",
    " --grad-checkpointing \\\n",
    " --log-every-n-steps 100 \\\n",
    " --grad-clip-norm 1 \\\n",
    " --data-key txt \\\n",
    " --lr 3e-4 \\\n",
    " --warmup 2000 \\\n",
    " --wd 0.1 \\\n",
    " --beta2 0.95 \\\n",
    " --epochs 100 \\\n",
    " --wandb-project-name open_lm_example \\\n",
    " --name open_lm_ex_$RANDOM \\\n",
    " --resume latest \\\n",
    " --logs ./logs/ \\\n",
    " --attn-name torch_attn \\\n",
    " --ffn-type swiglu_torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# switch to fsdp \n",
    "# \n",
    "/opt/conda/envs/open_lm/bin/torchrun --nproc-per-node 4 -m open_lm.main   \\\n",
    " --model open_lm_1b \\\n",
    " --train-data /home/ubuntu/Workspace/open_lm/preproc_data_old/2048-v1/0/shard-{0000000..0000004}.tar \\\n",
    " --train-num-samples 1000000000 \\\n",
    " --workers 8 \\\n",
    " --dataset-resampled \\\n",
    " --precision amp_bfloat16 \\\n",
    " --global-batch-size 4 \\\n",
    " --grad-checkpointing \\\n",
    " --log-every-n-steps 100 \\\n",
    " --grad-clip-norm 1 \\\n",
    " --data-key txt \\\n",
    " --lr 3e-4 \\\n",
    " --fsdp --fsdp-amp\\\n",
    " --warmup 2000 \\\n",
    " --wd 0.1 \\\n",
    " --beta2 0.95 \\\n",
    " --epochs 100 \\\n",
    " --wandb-project-name open_lm_example \\\n",
    " --name open_lm_ex_$RANDOM \\\n",
    " --resume latest \\\n",
    " --logs ./logs/ \\\n",
    " --attn-name torch_attn \\\n",
    " --ffn-type swiglu_torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# switch to fp8 \n",
    "# Not runnable - not enough gpu memory\n",
    "/opt/conda/envs/open_lm/bin/torchrun --nproc-per-node 1 -m open_lm.main   \\\n",
    " --model open_lm_1b \\\n",
    " --train-data /home/ubuntu/Workspace/open_lm/preproc_data_old/2048-v1/0/shard-{0000000..0000004}.tar \\\n",
    " --train-num-samples 1000000000 \\\n",
    " --workers 4 \\\n",
    " --dataset-resampled \\\n",
    " --precision amp_fp8 \\\n",
    " --global-batch-size 1 \\\n",
    " --grad-checkpointing \\\n",
    " --log-every-n-steps 100 \\\n",
    " --grad-clip-norm 1 \\\n",
    " --data-key txt \\\n",
    " --lr 3e-4 \\\n",
    " --warmup 2000 \\\n",
    " --wd 0.1 \\\n",
    " --beta2 0.95 \\\n",
    " --epochs 100 \\\n",
    " --wandb-project-name open_lm_example \\\n",
    " --name open_lm_ex_$RANDOM \\\n",
    " --resume latest \\\n",
    " --logs ./logs/ \\\n",
    " --attn-name torch_attn \\\n",
    " --ffn-type swiglu_torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2680 token/s,  18.386G\n",
    "\n",
    "/opt/conda/envs/open_lm/bin/torchrun --nproc-per-node 4 -m open_lm.main   \\\n",
    " --model open_lm_1b \\\n",
    " --dataset-type synthetic \\\n",
    " --train-num-samples 1000000000 \\\n",
    " --workers 8 \\\n",
    " --dataset-resampled \\\n",
    " --precision amp_bfloat16 \\\n",
    " --global-batch-size 4 \\\n",
    " --log-every-n-steps 20 \\\n",
    " --grad-clip-norm 1 \\\n",
    " --data-key txt \\\n",
    " --lr 3e-4 \\\n",
    " --fsdp --fsdp-amp\\\n",
    " --warmup 2000 \\\n",
    " --wd 0.1 \\\n",
    " --beta2 0.95 \\\n",
    " --epochs 100 \\\n",
    " --wandb-project-name open_lm_example \\\n",
    " --name open_lm_ex_$RANDOM \\\n",
    " --resume latest \\\n",
    " --logs ./logs/ \\\n",
    " --attn-name torch_attn \\\n",
    " --ffn-type swiglu_torch \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4433 token/s,  21.948G\n",
    "\n",
    "/opt/conda/envs/open_lm/bin/torchrun --nproc-per-node 4 -m open_lm.main   \\\n",
    " --model open_lm_1b \\\n",
    " --dataset-type synthetic \\\n",
    " --train-num-samples 1000000000 \\\n",
    " --workers 8 \\\n",
    " --dataset-resampled \\\n",
    " --precision amp_bfloat16 \\\n",
    " --global-batch-size 16 \\\n",
    " --log-every-n-steps 20 \\\n",
    " --grad-clip-norm 1 \\\n",
    " --data-key txt \\\n",
    " --lr 3e-4 \\\n",
    " --fsdp --fsdp-amp\\\n",
    " --warmup 2000 \\\n",
    " --wd 0.1 \\\n",
    " --beta2 0.95 \\\n",
    " --epochs 100 \\\n",
    " --wandb-project-name open_lm_example \\\n",
    " --name open_lm_ex_$RANDOM \\\n",
    " --resume latest \\\n",
    " --logs ./logs/ \\\n",
    " --attn-name torch_attn \\\n",
    " --ffn-type swiglu_torch \\\n",
    " --accum-freq 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4956 token/s,  16.894G\n",
    "\n",
    "/opt/conda/envs/open_lm/bin/torchrun --nproc-per-node 4 -m open_lm.main   \\\n",
    " --model open_lm_1b \\\n",
    " --dataset-type synthetic \\\n",
    " --train-num-samples 1000000000 \\\n",
    " --workers 8 \\\n",
    " --dataset-resampled \\\n",
    " --precision amp_bfloat16 \\\n",
    " --global-batch-size 16 \\\n",
    " --log-every-n-steps 10 \\\n",
    " --grad-clip-norm 1 \\\n",
    " --data-key txt \\\n",
    " --lr 3e-4 \\\n",
    " --fsdp --fsdp-pure-bf16 \\\n",
    " --fsdp-limit-all-gathers --fsdp-backward-prefetch \\\n",
    " --warmup 2000 \\\n",
    " --wd 0.1 \\\n",
    " --beta2 0.95 \\\n",
    " --epochs 100 \\\n",
    " --wandb-project-name open_lm_example \\\n",
    " --name open_lm_ex_$RANDOM \\\n",
    " --resume latest \\\n",
    " --logs ./logs/ \\\n",
    " --attn-name torch_attn \\\n",
    " --ffn-type swiglu_torch \\\n",
    " --accum-freq 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3729 token/s,  14.198G\n",
    "\n",
    "/opt/conda/envs/open_lm/bin/torchrun --nproc-per-node 4 -m open_lm.main   \\\n",
    " --model open_lm_1b \\\n",
    " --dataset-type synthetic \\\n",
    " --train-num-samples 1000000000 \\\n",
    " --workers 8 \\\n",
    " --dataset-resampled \\\n",
    " --precision amp_bfloat16 \\\n",
    " --global-batch-size 4 \\\n",
    " --log-every-n-steps 10 \\\n",
    " --grad-clip-norm 1 \\\n",
    " --data-key txt \\\n",
    " --lr 3e-4 \\\n",
    " --fsdp --fsdp-pure-bf16 \\\n",
    " --fsdp-limit-all-gathers --fsdp-backward-prefetch \\\n",
    " --warmup 2000 \\\n",
    " --wd 0.1 \\\n",
    " --beta2 0.95 \\\n",
    " --epochs 100 \\\n",
    " --wandb-project-name open_lm_example \\\n",
    " --name open_lm_ex_$RANDOM \\\n",
    " --resume latest \\\n",
    " --logs ./logs/ \\\n",
    " --attn-name torch_attn \\\n",
    " --ffn-type swiglu_torch \\\n",
    " --accum-freq 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7B MOdel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   NA token/s,  OOM\n",
    "\n",
    "/opt/conda/envs/open_lm/bin/torchrun --nproc-per-node 4 -m open_lm.main   \\\n",
    " --model open_lm_7b \\\n",
    " --dataset-type synthetic \\\n",
    " --train-num-samples 1000000000 \\\n",
    " --workers 8 \\\n",
    " --dataset-resampled \\\n",
    " --precision amp_bfloat16 \\\n",
    " --global-batch-size 4 \\\n",
    " --log-every-n-steps 10 \\\n",
    " --grad-clip-norm 1 \\\n",
    " --data-key txt \\\n",
    " --lr 3e-4 \\\n",
    " --fsdp --fsdp-pure-bf16 \\\n",
    " --fsdp-limit-all-gathers --fsdp-backward-prefetch \\\n",
    " --warmup 2000 \\\n",
    " --wd 0.1 \\\n",
    " --beta2 0.95 \\\n",
    " --epochs 100 \\\n",
    " --wandb-project-name open_lm_example \\\n",
    " --name open_lm_ex_$RANDOM \\\n",
    " --resume latest \\\n",
    " --logs ./logs/ \\\n",
    " --attn-name torch_attn \\\n",
    " --ffn-type swiglu_torch \\\n",
    " --accum-freq 1\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
